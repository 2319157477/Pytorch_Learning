# 第五课 逻辑斯蒂回归

> 实现分类问题

对于分类问题，比如对MNIST数据集的训练，由于数字之间分类不遵循实数大小的关系（7和9更相似，但7和8在实数域上更接近），所以不能使用简单的线性模型来训练

## 1. 逻辑斯蒂函数（Logistic Function）

$$
f(x)=\frac{1}{1+e^{-x}}
$$



### 特点

1. **S型曲线**：逻辑斯蒂函数的图像是一条S型曲线，表示从0到1的平滑过渡。
2. **输出范围**：该函数的输出值范围在0到1之间，因此常用于二分类问题中的概率预测。
3. **中心对称**：逻辑斯蒂函数在点(0,0.5)(0, 0.5)(0,0.5)处对称。

### 应用

1. **机器学习中的二分类问题**：逻辑回归模型使用逻辑斯蒂函数来预测样本属于某一类别的概率。
2. **神经网络中的激活函数**：在深度学习中，逻辑斯蒂函数作为激活函数的一种，用于引入非线性特性。

### 优点和缺点

**优点**：

- 平滑过渡，有助于捕捉输入变量的非线性关系。
- 输出范围固定，有利于概率估计。

**缺点**：

- 在极端值处（非常大或非常小的输入值），梯度趋于零，可能导致梯度消失问题。
- 对于多分类问题，需要扩展到Softmax函数。

## 2.在线性模型后引入逻辑斯蒂模型

1. **变更forward**

![image-20240731211119756](https://raw.githubusercontent.com/2319157477/img_bed/main/img/image-20240731211119756.png)

2. **变更loss（BCE损失）**

   ![image-20240731212043841](https://raw.githubusercontent.com/2319157477/img_bed/main/img/image-20240731212043841.png)

   ## 3. 代码实现

   ~~~python
   import torch
   import torch.nn.functional as tf ##引入函数包
   
   x_data = torch.Tensor([[1.0], [2.0], [3.0]])
   y_data = torch.Tensor([[0], [0], [1]])
   epochs = 5000
   
   class LogisticRegressionModel(torch.nn.Module):
       def __init__(self):
           super().__init__()
           self.linear = torch.nn.Linear(1, 1)
   
       def forward(self, x):
           #更改为sigmoid（逻辑斯蒂）函数
           y_pred = tf.sigmoid(self.linear(x))
           return y_pred
       
   model = LogisticRegressionModel()
   
   #损失函数变更为BCE（交叉熵损失）
   criterion = torch.nn.BCELoss(reduction="sum")
   #优化器选择Adam
   optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
   
   #训练
   for epoch in range(epochs):
       y_pred = model(x_data)
       loss = criterion(y_pred, y_data)
       if (epoch + 1) % 100 == 0:
           print(f'Epoch {epoch + 1}: loss = {loss.item()}')
   
       #优化器归零
       optimizer.zero_grad()
       #对loss反向传播
       loss.backward()
       #使用优化器对权重进行一次更新
       optimizer.step()
   
   #测试
   x_test = torch.Tensor([[4.0]])
   print('y_pred = ', model(x_test).data)
   ~~~

   