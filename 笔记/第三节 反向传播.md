# 第三节 反向传播

 ## 1. 引入

在前几讲中，我们设计了一个线性模型：
$$
\hat{y}=x*\omega
$$
![image-20240730155116947](https://raw.githubusercontent.com/2319157477/img_bed/main/img/image-20240730155116947.png)

对于这种简单的模型，我们可以手动设计梯度，但如果是下面这种复杂的模型，每一个权重都有着很多的嵌套，几乎不可能人工设计梯度的解析式。对于这种情况，我们是否可以设计一种算法，根据链式法则来得出解析式？这就是反向传播算法（Backpropagation）

## 2. 算法原理

对于每一层（layer），我们进行如下处理：

![image-20240730162539288](https://raw.githubusercontent.com/2319157477/img_bed/main/img/image-20240730162539288.png)

与权重（Weight）进行矩阵乘法，与偏置（Bias）进行矩阵加法，这称之为“一层”。

但是，当线性模型的矩阵有多层时
$$
\hat{y}=W_2(W_1\cdot X+b_1)+b_2
$$
可以被化为：
$$
\hat{y}=W_2(W_1\cdot X+b_1)+b_2\\
=W_2\cdot W_1\cdot X+(W_2b_1+b_2)\\
=W\cdot X + b
$$
从而使得网络不管有多少层，都与单层线性模型无本质区别。所以要引入激活函数（Nonlinear Function）：

![image-20240730163436728](https://raw.githubusercontent.com/2319157477/img_bed/main/img/image-20240730163436728.png)

激活函数引入了非线性，使神经网络能够处理复杂的非线性关系，从而增强其表达能力。

运算过程：

**1. 正向传播（Forward Propagation）**：

- 输入数据通过网络的各层传播，计算每个神经元的输出，直到最终得到网络的输出。

**2. 计算误差（Error Calculation）**：

- 计算网络输出与期望输出之间的误差，通常使用均方误差（Mean Squared Error）或交叉熵损失（Cross-Entropy Loss）等。

**3. 反向传播误差（Backward Propagation of Error）**：

- 从输出层开始，逐层向后计算误差的梯度。这个过程使用链式法则（Chain Rule）来计算每个权重的偏导数。
- 计算各层的梯度时，首先计算输出层的误差，然后逐层向前计算每一层的误差，直到输入层。

**4. 更新权重（Weight Update）**：

- 使用梯度下降算法（Gradient Descent）或其变种（如随机梯度下降、动量法、Adam等）根据计算出的梯度调整每个权重。

- 权重更新公式一般为：
  $$
  w_{ij}= w_{ij} - \eta \frac{\partial L}{\partial w_{ij}}\\
   其中，w_{ij}是权重，η是学习率，L是损失函数。
  $$

反向传播算法通过**多次迭代**上述步骤，不断调整网络的权重，使得损失函数逐渐减小，从而使网络的输出逐渐接近期望输出。这一过程也称为**神经网络的训练**。

![image-20240730170758383](https://raw.githubusercontent.com/2319157477/img_bed/main/img/image-20240730170758383.png)



![image-20240730173521880](https://raw.githubusercontent.com/2319157477/img_bed/main/img/image-20240730173521880.png)

## 3.代码实现

对于
$$
\hat{y} = w_1x^2+w_2x+b
$$
的反向传播计算

~~~python
import torch
import matplotlib.pyplot as plt

x_data = [1.0, 2.0, 3.0, 4.0, 5.0]
y_data = [8.0, 19.0, 36.0, 59.0, 88.0]

#学习率
lr = 0.0005
epochs = 10000

#权重1
w_1 = torch.tensor([1.0], requires_grad = True)
#权重2
w_2 = torch.tensor([1.0], requires_grad = True)
#偏置
b = torch.tensor([1.0], requires_grad = True)

#此处"*"代表Tensor之间的数乘
def forward(x):
    return x * x * w_1 + x * w_2 + b

#这个函数实际构建了一个计算图
def loss(x, y):
    y_pred = forward(x)
    return (y_pred - y) ** 2

print("训练前的预测：", 4, forward(6).item())
# 用于存储每个 epoch 的损失值
loss_values = []

for epoch in range(epochs):
    for x, y in zip(x_data, y_data):
        l = loss(x, y)
        l.backward() #计算图在进行反向传播后即被释放
        #print('\tgrad:', x, y, w.grad.item())
        with torch.no_grad():
            w_1 -= lr * w_1.grad
            w_2 -= lr * w_2.grad
            b -= lr * b.grad
        w_1.grad.zero_() #将梯度清零
        w_2.grad.zero_()
        b.grad.zero_() 

    loss_values.append(l.item())
    print("Epoch:", epoch + 1, "loss:", l.item())

print("训练后的预测：", 4, forward(6).item())

plt.plot(range(1, epochs + 1), loss_values)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss By Epoch')
plt.show()
~~~

结果展示：

![image-20240730200137495](https://raw.githubusercontent.com/2319157477/img_bed/main/img/image-20240730200137495.png)



