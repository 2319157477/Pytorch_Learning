# 第六课 处理多维特征的输入

## 1. 数据处理过程

![image-20240731225541417](https://raw.githubusercontent.com/2319157477/img_bed/main/img/image-20240731225541417.png)

在进行计算时，Pytorch的sigmoid函数实际上被视为按元素计算的形式，我们可以将计算化为矩阵的运算

![image-20240731230242997](https://raw.githubusercontent.com/2319157477/img_bed/main/img/image-20240731230242997.png)

可以将矩阵的本质理解为**空间变化函数**，在Pytorch中Linear（）的作用可以理解为将数据原本的维度映射到不同的维度上去

![](https://raw.githubusercontent.com/2319157477/img_bed/main/img/image-20240731231521899.png)

进行多次线性变换，最后得到想要的数据维度

![image-20240731232755156](https://raw.githubusercontent.com/2319157477/img_bed/main/img/image-20240731232755156.png)

通常的，更多的层数更有利于神经网络捕捉样本的信息，但同时会更多的学习到样本的噪声，从而导致过拟合，使模型的泛化能力下降。

## 2. 训练流程

与前几课的训练流程类似

![image-20240731233216112](https://raw.githubusercontent.com/2319157477/img_bed/main/img/image-20240731233216112.png)

1. **准备数据集**

   ![image-20240731233303367](https://raw.githubusercontent.com/2319157477/img_bed/main/img/image-20240731233303367.png)

   

2. **构建模型**

   ![image-20240731234911822](https://raw.githubusercontent.com/2319157477/img_bed/main/img/image-20240731234911822.png)

3. **构建损失函数和优化器**

   ![image-20240731235055596](https://raw.githubusercontent.com/2319157477/img_bed/main/img/image-20240731235055596.png)

   

4. **训练周期**

   ![](https://raw.githubusercontent.com/2319157477/img_bed/main/img/image-20240731235658477.png)3. 激活函数选择

   
## 3. 代码实现

~~~python
import numpy as np
import torch
import torch.optim as optim

xy = np.loadtxt('diabetes.csv.gz', delimiter=',', dtype=np.float32)
x_data = torch.from_numpy(xy[:,:-1]) #[:,:-1]中，第一个':'表示所有行,':-1'表示排除-1这一行(即y)
y_data = torch.from_numpy(xy[:, [-1]]) #[:, [-1]]中,[-1]表示只需要最后一行,并且存储为矩阵形式
epochs = 500000

class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear1 = torch.nn.Linear(8, 6) #8维降到6维
        self.linear2 = torch.nn.Linear(6, 4)
        self.linear3 = torch.nn.Linear(4, 1)
        self.activate = torch.nn.ReLU() #引入激活函数
        self.sigmoid = torch.nn.Sigmoid()

    def forward(self, x):
        x = self.activate(self.linear1(x)) #进行第一步变换,并且使用非线性激活函数
        x = self.activate(self.linear2(x))
        x = self.sigmoid(self.linear3(x))
        return x

model = Model() 

#损失函数为BCE（交叉熵损失）
criterion = torch.nn.BCELoss(reduction="sum")
#优化器选择Adam
optimizer = optim.Adam(model.parameters(), lr=0.1)
#学习率下降
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100000, gamma=0.1)

for epoch in range(epochs):
    #Forward
    y_pred = model(x_data)
    loss = criterion(y_pred, y_data)
    if (epoch + 1) % 100 == 0:
        print('Epoch:', epoch + 1, 'Loss:', loss.data.item())
    
    #Backward
    optimizer.zero_grad()
    loss.backward()

    #Update
    optimizer.step()
    scheduler.step() 
~~~



