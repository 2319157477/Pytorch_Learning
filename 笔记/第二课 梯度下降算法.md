# 第二课 梯度下降算法

## 1. 引入



​	在上一讲中，我们使用**均方误差MSE**作为cost函数：
$$
cost=\frac{1}{N}\sum_{n=1}^N (\hat{y_n}-y_n)
$$
​	现在, 我们想要确定w最小值到底出现在什么地方，于是引入优化问题（Optimization Problem）：
$$
𝜔_∗= arg min_𝜔𝑐𝑜𝑠𝑡(𝜔)
$$
​	![图一](C:\Users\23191\AppData\Roaming\Typora\typora-user-images\image-20240728192242017.png)

​	为了实现这一需求，我们使用梯度下降算法(Gradient Descent Algorithm)

## 2. 算法简析

​	对于一个图像，我们先取起始点：

![image-20240728192932079](C:\Users\23191\AppData\Roaming\Typora\typora-user-images\image-20240728192932079.png)

​	如果我们想要在接下来的过程中向最低点迫近，可以对求cost函数在起始点上的导数

​	![image-20240728193128260](C:\Users\23191\AppData\Roaming\Typora\typora-user-images\image-20240728193128260.png)

​	此时，向着导数的负方向前进，就是在逼近函数的最小值

​	于是，我们设计一个学习率（learning rate）a，并且设置update(w)的方法

![image-20240728193457714](C:\Users\23191\AppData\Roaming\Typora\typora-user-images\image-20240728193457714.png)

​	但是，如果函数为非凸函数，梯度下降算法只能找出局部最优点，类似下图情况：

![image-20240728204718507](C:\Users\23191\AppData\Roaming\Typora\typora-user-images\image-20240728204718507.png)

​	计算update函数

![image-20240728213037781](C:\Users\23191\AppData\Roaming\Typora\typora-user-images\image-20240728213037781.png)

## 3. 代码实现

~~~python
import numpy as np;
import matplotlib.pyplot as plt;
from mpl_toolkits.mplot3d import Axes3D;

w = 1.0 #初始权重
b = 2 #初始偏置
lr = 0.01 #学习率
x_data = [1.0, 2.0, 3.0, 4.0, 5.0]
y_data = [5.0, 8.0, 11.0, 14.0, 17.0]

#表示模型
def forward(x) :
    return x * w + b

#损失函数
def cost(xs, ys):
    cost = 0
    for x, y in zip(xs, ys):
        y_pred = forward(x)
        cost += (y_pred - y) ** 2
    return cost / len(xs)

#表示梯度
def gradient(xs, ys):
    grad = 0
    for x, y in zip(xs, ys):
        grad += 2 * x * (x * w + b - y)
    return grad / len(xs)

#下降过程
print("训练前预测：", 4, forward(4))

for epoch in range(10000):
    cost_val = cost(x_data, y_data)
    grad_val = gradient(x_data, y_data)
    w -= lr * grad_val
    print("Epoch: ", epoch + 1, "w = ", w, "loss = ", cost_val)

print("训练后预测：", 4, forward(4))
~~~

结果：

![image-20240728221759435](C:\Users\23191\AppData\Roaming\Typora\typora-user-images\image-20240728221759435.png)

## 4. 随机梯度下降

为了解决梯度下降算法对于鞍点的处理问题，可以使用随机梯度下降算法：

![image-20240728221926385](C:\Users\23191\AppData\Roaming\Typora\typora-user-images\image-20240728221926385.png)

即随机使用单个loss函数进行训练

代码实现：

~~~python
import numpy as np;
import matplotlib.pyplot as plt;
from mpl_toolkits.mplot3d import Axes3D;
import random as rd

w = 1.0 #初始权重
b = 2 #初始偏置
lr = 0.01 #学习率
x_data = [1.0, 2.0, 3.0, 4.0, 5.0]
y_data = [5.0, 8.0, 11.0, 14.0, 17.0]

#表示模型
def forward(x) :
    return x * w + b

#损失函数
def loss(x, y):
    y_pred = forward(x)
    loss = (y_pred - y) ** 2
    return loss
    

#表示梯度
def gradient(x, y):
    grad = 2 * x * (x * w + b - y)
    return grad

#下降过程
print("训练前预测：", 4, forward(4))

for epoch in range(1000):
    for x, y in zip(x_data, y_data):
        grad = gradient(x, y)
        r = rd.randint(0, 2) #体现随机过程（实际应该随机选择样本）
        if (r == 1): w -= lr * grad
        l = loss(x, y)
    print("Epoch: ", epoch + 1, "w = ", w, "loss = ", l)

print("训练后预测：", 4, forward(4))
~~~

结果：

​	![image-20240728232348571](C:\Users\23191\AppData\Roaming\Typora\typora-user-images\image-20240728232348571.png)



​	**梯度下降算法**每次迭代中使用**整个训练数据集**来计算损失函数的梯度，然后更新模型的参数。由于计算梯度需要**遍历**整个数据集，这一步骤非常**适合并行化**。：而**随机梯度下降算法**每次更新模型参数都**依赖于**前一次更新的结果，这种串行依赖性使得**很难并行化**。每个样本的梯度计算和参数更新都必须按顺序进行。这种串行依赖性使得很难并行化。每个样本的梯度计算和参数更新都必须**按顺序进行**。所以，为了兼顾性能与效果，现在一般使用**小批量梯度下降（Mini-batch Gradient Descent）**技术:

> **小批量并行**：虽然小批量的大小小于整个数据集，但大于单个样本，这使得可以在硬件上并行计算每个小批量的梯度。

> **平衡频率和规模**：小批量梯度下降在并行计算和频繁更新之间取得了平衡，既能利用现代硬件的并行计算能力，又避免了SGD的频繁更新问题。

# 总结：

**梯度下降算法（Batch Gradient Descent）** 由于使用整个数据集来计算梯度，非常适合并行化处理，可以利用现代硬件的计算能力来加速。

**随机梯度下降算法（SGD）** 每次迭代只使用一个样本来更新参数，由于其强依赖性和频繁更新，难以有效并行化。

**小批量梯度下降（Mini-batch Gradient Descent）** 在并行计算和频繁更新之间取得了平衡，适合现代硬件的计算特性。

